# Cache
## 为什么引入cache
由于内存墙的存在，在CPU和主存之间增加cache来提高效率<br>
cache中存放了主存部分数据的副本<br>
## cache的工作流程
1.检查(check):检查某个字是否在cache中<br>
2.检查后分两种情况处理：<br>
命中(hit):直接返回CPU<br>
未命中(miss):将主存中包含该字的块加载到cache然后返回CPU<br>
## 如何判断是否命中
Cache通过tag来标识其内容在主存中的对应位置<br>
## 程序访问的局部性原理
时间局部性:在相对较短的时间周期内，重复访问特定的信息<br>
空间局部性:在相对较短的时间周期内，访问相邻存储位置的数据<br>
## 平均访问时间
设p是命中率，Tc是cache的访问时间，Tm是主存的访问时间<br>
则平均访问时间为p*Tc+(1-p)*Tm<br>
## cache未命中原因
义务失效<br>
容量失效<br>
冲突失效<br>
## 映射功能
实现主存块到cache行的映射<br>
映射方式主要有三种：直接映射、关联映射、组关联映射<br>
### 直接映射
将主存中的每一个块映射到一个固定可用的cache行中<br>
若i为cache行号，j是主存块块号，C是cache的行数，那么 i=j mod C<br>
主存地址划分为三个部分：标记、cache行号和块内地址<br>
其中cache行号占log2(C)位，块内地址占log2(M)位，M为每块的字节数(字数)<br>
#### 优点
简单、快速映射、快速检查
#### 缺点
抖动现象
### 关联映射
一个主存块可以存入cache任意一行<br>
主存地址划分为标记和块内地址<br>
标记占log2(M)位，M为主存块数，剩下的为块内地址
#### 优点
避免抖动
#### 缺点
实现起来复杂<br>
cache搜索代价大<br>
### 组关联映射
cache分为若干组，每组包含相同数量的行，每个主存块被映射到固定组的任意一行<br>
若s为cache组号，j是主存块号，S是组数，则<br>
s = j mod S<br>
K路组关联映射：K = C / S (C为行数，S为组数)<br>
主存地址划分为标记、组号和块内地址<br>
组号占log2(S)位，块内地址占log2(M)位，剩下的是标记<br>
结合了直接映射和关联映射的优缺点<br>
## 替换算法
当cache行被占用，新的数据装入cache中时，原先存放的数据块将会被替换掉<br>
设计替换算法的目的是提高命中率<br>
### 常用的替换算法
#### LRU算法
替换掉cache中最长时间未被访问的数据块<br>
每行包含一个USE位<br>
当同一组的某行被访问时，将其USE位置为1，同时其他行的USE位向下顺移<br>
当新的数据块读入该组时，替换掉USE为0的行<br>
LRU实现需要额外的硬件实现：K路需要约klogk位<br>
#### FIFO算法
替换掉在cache中停留时间最长的块<br>
当同一组的某行被替换时，将其标识位设为1，同时将下一行的标识位设为0<br>
当新的数据块读入该组时，替换掉标识位为0的行<br>
仅当替换时需要改变标识位<br>
#### LFU算法
替换掉cache中被访问次数最少的数据块<br>
#### Random算法
随机替换cache中的数据块<br>
## 写策略
### 写直达（Write-through）
所有写操作都同时对cache和主存进行<br>
### 写回 （Write-back）
先更新cache中的数据，当它要被替换时，再更新主存<br>
利用一个脏位(dirty bit)来表示块是否被修改<br>
## 行大小
行大小从1逐渐增大，cache的命中率会增加(利用了空间局部性)<br>
在行大小变得较大后，继续增加行大小，则cache命中率会下降<br>
### 行大小和命中率的关系
行太小，行数太多反时间局部性<br>
行太大，行数太少反空间局部性<br>
## Cache数目
### 一级 VS 多级
### 统一 VS 分立
